{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qMoqsO8JmRta"
   },
   "outputs": [],
   "source": [
    "import zipfile, os, cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "tf.executing_eagerly()\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow import keras\n",
    "# Display\n",
    "# from IPython.display import Image\n",
    "from keras import backend as K\n",
    "import gc \n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tTz4-RjAnP-T"
   },
   "outputs": [],
   "source": [
    "cv=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oXOCcVo8m6qq",
    "outputId": "46c5d7b5-c27a-4791-be12-ea8e2e87b0ac"
   },
   "outputs": [],
   "source": [
    "save_cv_data_path= '../nii_save_np/lacune/'\n",
    "\n",
    "np_image = np.load(save_cv_data_path+f'cv{cv}_easy_x_strain_val.npy')\n",
    "np_masks = np.load(save_cv_data_path+f'cv{cv}_easy_y_strain_val.npy')\n",
    "path = np.load(save_cv_data_path+f'cv{cv}_easy_path_train_val.npy')\n",
    "print(np_image.shape, np_masks.shape, path.shape)\n",
    "\n",
    "print(np_image.shape, np_masks.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7h6lM7OnnfVZ"
   },
   "outputs": [],
   "source": [
    "ds=int(np_image.shape[0]*0.8)# data_scale\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ax-F6z2endFc"
   },
   "outputs": [],
   "source": [
    "x_train = np_image[:ds].astype(np.float32)\n",
    "y_train = np_masks[:ds]\n",
    "x_val = np_image[ds:].astype(np.float32)\n",
    "y_val = np_masks[ds:]\n",
    "path_train = path[ds:]\n",
    "path_val = path[:ds]\n",
    "x_train = np.reshape(x_train, (x_train.shape[0]*x_train.shape[1],192,192,1))\n",
    "y_train = np.reshape(y_train, (y_train.shape[0]*y_train.shape[1],192,192,1))\n",
    "x_val = np.reshape(x_val, (x_val.shape[0]*x_val.shape[1],192,192,1))\n",
    "y_val = np.reshape(y_val, (y_val.shape[0]*y_val.shape[1],192,192,1))\n",
    "print(x_train.shape, y_train.shape, x_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def del_darkimg(img,mask):\n",
    "    temp_i = []\n",
    "    temp_m = []\n",
    "    for i in range(img.shape[0]):\n",
    "        if np.sum(img[i])!=0:\n",
    "            temp_i.append(img[i])\n",
    "            temp_m.append(mask[i])\n",
    "    temp = np.array(temp_i)\n",
    "    temp2 = np.array(temp_m)\n",
    "    return temp, temp2\n",
    "X_train, Y_train = del_darkimg(x_train, y_train)\n",
    "X_val, Y_val = del_darkimg(x_val, y_val)\n",
    "print(X_train.shape, Y_train.shape, X_val.shape, Y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data augmentation and data zip\n",
    "from ImageDataAugmentor.image_data_augmentor import *\n",
    "import albumentations as A\n",
    "\n",
    "AUGMENTATIONS = A.Compose([\n",
    "    A.Rotate(p=0.5),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.RandomBrightnessContrast(p=0.2),\n",
    "    A.ShiftScaleRotate(p=0.5)\n",
    "],)  \n",
    "\n",
    "\n",
    "\n",
    "# dataloaders\n",
    "img_data_gen = ImageDataAugmentor(augment=AUGMENTATIONS, input_augment_mode='image', seed=123)\n",
    "mask_data_gen = ImageDataAugmentor(augment=AUGMENTATIONS, input_augment_mode='mask',seed=123,)\n",
    "\n",
    "image_data_valid = ImageDataAugmentor(augment=None, input_augment_mode='image',seed=123)\n",
    "mask_data_valid = ImageDataAugmentor(augment=None, input_augment_mode='mask',seed=123)\n",
    "\n",
    "batch_size=32\n",
    "X_train_datagen = img_data_gen.flow(X_train, batch_size=batch_size)\n",
    "Y_train_datagen = mask_data_gen.flow(Y_train, batch_size=batch_size)\n",
    "train_generator = zip(X_train_datagen, Y_train_datagen)\n",
    "\n",
    "X_valid_datagen = image_data_valid.flow(X_val, batch_size=batch_size)\n",
    "Y_valid_datagen = mask_data_valid.flow(Y_val, batch_size=batch_size)\n",
    "valid_generator = zip(X_valid_datagen, Y_valid_datagen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "upjS7dBImRtz"
   },
   "outputs": [],
   "source": [
    "def dice_coef(y_true, y_pred):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection) / (K.sum(y_true_f) + K.sum(y_pred_f))\n",
    "\n",
    "def DBCEL(targets, inputs, smooth=1e-6):\n",
    "    dice_loss = 1 - dice_coef(targets, inputs)\n",
    "    BCE =  binary_crossentropy(targets, inputs)\n",
    "    DBCE = BCE + dice_loss\n",
    "    \n",
    "    return DBCE\n",
    "\n",
    "def FTL(targets, inputs, alpha=0.3, beta=0.7, gamma=2, smooth=1e-6):\n",
    "    \n",
    "        #flatten label and prediction tensors\n",
    "        inputs = K.flatten(inputs)\n",
    "        targets = K.flatten(targets)\n",
    "        \n",
    "        #True Positives, False Positives & False Negatives\n",
    "        TP = K.sum((inputs * targets))\n",
    "        FP = K.sum(((1-targets) * inputs))\n",
    "        FN = K.sum((targets * (1-inputs)))\n",
    "               \n",
    "        Tversky = (TP + smooth) / (TP + alpha*FP + beta*FN + smooth)  \n",
    "        FocalTversky = K.pow((1 - Tversky), gamma)\n",
    "        return FocalTversky\n",
    "loss = DBCEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ajcT5vwGmRt0",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%env SM_FRAMEWORK=tf.keras\n",
    "from segmentation_models import Unet\n",
    "import segmentation_models as sm\n",
    "from keras.optimizers import Adam\n",
    "from tensorflow_addons.optimizers import AdamW\n",
    "from keras import backend as K\n",
    "from keras.metrics import binary_crossentropy\n",
    "# fitting shape [[slice, w, h, c], class]\n",
    "img_rows=192\n",
    "img_cols=192\n",
    "lr = 1e-3\n",
    "epochs = 80\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k05ZBwdjmRt0"
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint, TensorBoard, EarlyStopping, LearningRateScheduler\n",
    "weight_dir = 'checkpoint/seg_package/'\n",
    "checkpoint_name = 'test'\n",
    "# checkpoint_name = f'154_2DDense121_binary_batch-32_DBCEL_leaky_aug_norm-01_LRS_Adam-lr-{lr}_cv{cv}_1'\n",
    "logdir = os.path.join(\"checkpoint/seg_package_tensorboard/\", checkpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Learning Rate Scheduler\n",
    "class SnapshotCallbackBuilder:\n",
    "    def __init__(self, nb_epochs, nb_snapshots, init_lr=0.1):\n",
    "        self.T = nb_epochs\n",
    "        self.M = nb_snapshots\n",
    "        self.alpha_zero = init_lr\n",
    "\n",
    "    def get_callbacks(self, model_prefix='Model'):\n",
    "\n",
    "        callback_list = [\n",
    "            ModelCheckpoint(os.path.join(weight_dir,f\"{checkpoint_name}.hdf5\"), \n",
    "                                   monitor='val_loss', mode=\"min\", verbose=1, save_best_only=True),\n",
    "            TensorBoard(log_dir=logdir,histogram_freq=1,\n",
    "                                         embeddings_freq=0,embeddings_layer_names=None,),\n",
    "#             EarlyStopping(monitor=\"val_loss\", patience=15, verbose=1),\n",
    "#             swa,\n",
    "            LearningRateScheduler(schedule=self._cosine_anneal_schedule)\n",
    "        ]\n",
    "\n",
    "        return callback_list\n",
    "\n",
    "    def _cosine_anneal_schedule(self, t):\n",
    "        cos_inner = np.pi * (t % (self.T // self.M))  # t - 1 is used when t has 1-based indexing.\n",
    "        cos_inner /= self.T // self.M\n",
    "        cos_out = np.cos(cos_inner) + 1\n",
    "        return float(self.alpha_zero / 2 * cos_out)\n",
    "\n",
    "# Stochastic Weight Averaging\n",
    "class SWA(keras.callbacks.Callback):\n",
    "    \n",
    "    def __init__(self, filepath, swa_epoch):\n",
    "        super(SWA, self).__init__()\n",
    "        self.filepath = filepath\n",
    "        self.swa_epoch = swa_epoch \n",
    "    \n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.nb_epoch = self.params['epochs']\n",
    "        print('Stochastic weight averaging selected for last {} epochs.'\n",
    "              .format(self.nb_epoch - self.swa_epoch))\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        \n",
    "        if epoch == self.swa_epoch:\n",
    "            self.swa_weights = self.model.get_weights()\n",
    "            \n",
    "        elif epoch > self.swa_epoch:    \n",
    "            for i in range(len(self.swa_weights)):\n",
    "                self.swa_weights[i] = (self.swa_weights[i] * \n",
    "                    (epoch - self.swa_epoch) + self.model.get_weights()[i])/((epoch - self.swa_epoch)  + 1)  \n",
    "\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "    def on_train_end(self, logs=None):\n",
    "        self.model.set_weights(self.swa_weights)\n",
    "        print('Final model parameters set to stochastic weight average.')\n",
    "        self.model.save_weights(self.filepath)\n",
    "        print('Final stochastic averaged weights saved to file.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xHJ4dwpbD5LX",
    "outputId": "9d58392a-96ac-454d-d6fe-7a38a6831f8e"
   },
   "outputs": [],
   "source": [
    "snapshot = SnapshotCallbackBuilder(nb_epochs=epochs,nb_snapshots=1,init_lr=lr)\n",
    "swa = SWA(weight_dir + checkpoint_name+'_swa', int(epochs*0.9))\n",
    "\n",
    "model = Unet(backbone_name='densenet121', encoder_weights=None, input_shape=(img_rows, img_cols, 1))\n",
    "\n",
    "optimizer = 'adam'\n",
    "model.compile(optimizer=optimizer, \n",
    "              loss=loss, \n",
    "              metrics=['accuracy'\n",
    "                       ,dice_coef\n",
    "                       ,tf.keras.metrics.MeanIoU(num_classes=2, name='iou')\n",
    "                       ,tf.keras.metrics.AUC()\n",
    "                       ,tf.keras.metrics.Recall()])\n",
    "\n",
    "print(weight_dir+checkpoint_name)\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f94ZkHZImRt0",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('-'*30,'\\nFitting model...\\n','-'*30)\n",
    "\n",
    "history = model.fit(train_generator, epochs=epochs, \n",
    "                    verbose=1,\n",
    "                    steps_per_epoch= len(y_train)/32*5,\n",
    "                    shuffle=True, validation_data=valid_generator,\n",
    "                    validation_steps= len(y_val)/32,          \n",
    "                    callbacks=snapshot.get_callbacks()\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "arVRgw68mRt1",
    "outputId": "861842a6-4518-4555-aea8-547be75f542f"
   },
   "outputs": [],
   "source": [
    "print('-'*30)\n",
    "print('Loading and preprocessing test data...')\n",
    "print('-'*30)\n",
    "\n",
    "\n",
    "imgs_test = X_val\n",
    "imgs_mask_test = Y_val\n",
    "\n",
    "\n",
    "print('-'*30)\n",
    "print('Loading saved weights...')\n",
    "print('-'*30)\n",
    "\n",
    "model = Unet(backbone_name='densenet121', encoder_weights=None, input_shape=(img_rows, img_cols, 1))\n",
    "checkpoint_names = weight_dir+checkpoint_name + '.hdf5'\n",
    "print(checkpoint_names)\n",
    "model.load_weights(checkpoint_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ERdFRAplmRt1",
    "outputId": "4635d9a4-76fb-475d-bbaa-8d37bde9b59a"
   },
   "outputs": [],
   "source": [
    "Results = model.predict(imgs_test, batch_size=1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aKiH4yzAmRt1",
    "outputId": "3a8ebb62-50fa-475f-f929-7ba434883e12"
   },
   "outputs": [],
   "source": [
    "print(Results.shape)\n",
    "# print(np.array(sorted(path_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bfeO3jIoMWmI"
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import requests\n",
    "flatten_pred = np.reshape(Results,(Results.shape[0]*192*192,Results.shape[-1]))\n",
    "flatten_truth = np.reshape(imgs_mask_test,(imgs_mask_test.shape[0]*192*192,imgs_mask_test.shape[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qMpOe6dNM3Xe"
   },
   "outputs": [],
   "source": [
    "# print(flatten_pred.shape, flatten_truth.shape)\n",
    "argmax_pred = np.reshape((flatten_pred>0.10).astype(int),(len(flatten_pred)))\n",
    "argmax_truth = np.reshape(flatten_truth, (flatten_truth.shape[0])).astype(int)\n",
    "# print(argmax_pred[0:5], argmax_truth[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ofQPOUkeNG1H",
    "outputId": "941fcc19-1c76-4236-dead-2991e0d7bb0c"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix\n",
    "cm = confusion_matrix(argmax_truth, argmax_pred)\n",
    "np.set_printoptions(precision=2)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_FjnmDKgOlsa"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=True,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title, fontsize=25)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=90,fontsize=15)\n",
    "    plt.yticks(tick_marks, classes,fontsize=15)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\",\n",
    "                 fontsize=20)\n",
    "    \n",
    "    plt.ylabel('True label', fontsize=20)\n",
    "    plt.xlabel('Predicted label',fontsize=20)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 440
    },
    "id": "GdllX-ytOpxS",
    "outputId": "e1b61fcf-fdf2-407e-a19c-41e6e2e1cac5"
   },
   "outputs": [],
   "source": [
    "species = ['Non-Stroke', 'Stroke']\n",
    "plt.figure(figsize=(6,6))\n",
    "plot_confusion_matrix(cm, species)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "7mLRT88lmRt2",
    "outputId": "e699368c-e5f7-4493-9c46-fc1d5f027ce0",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# for j in range(Results.shape[0]):\n",
    "#     count = 1\n",
    "#     for i in range(32):\n",
    "#         if np.max(imgs_test[j][i])>0:\n",
    "            \n",
    "#             plt.figure(figsize=(12,12))\n",
    "#             plt.subplot(1,3,1)\n",
    "#             plt.imshow(np.squeeze(imgs_test[j][i]), cmap='gray')\n",
    "#             plt.title('Original Image')\n",
    "#             plt.subplot(1,3,2)\n",
    "#             plt.imshow(np.squeeze(imgs_mask_test[j][i]), cmap='gray')\n",
    "#             plt.title('Original Mask')\n",
    "#             plt.subplot(1,3,3)\n",
    "#             plt.imshow(np.squeeze(Results[j][i]) > .5, cmap='gray')\n",
    "\n",
    "#             plt.title('Prediction')\n",
    "#             plt.show()\n",
    "#             print(valid_path[j][-14:]+\"_\"+str(count)+\"↑\")\n",
    "#             count+=1"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "MRI_3DUnet_Segmentation_for_binary_class.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
